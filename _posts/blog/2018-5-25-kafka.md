---
layout: post
title: kafka相关知识总结（二）
categories: 消息队列
description: 
keywords: 
---

### kafka高可用

#### 为何需要Replication

　　在Kafka在0.8以前的版本中，是没有Replication的，一旦某一个Broker宕机，则其上所有的Partition数据都不可被消费，这与Kafka数据持久性及Delivery Guarantee的设计目标相悖。同时Producer都不能再将数据存于这些Partition中。

- 如果Producer使用同步模式则Producer会在尝试重新发送`message.send.max.retries`（默认值为3）次后抛出Exception，用户可以选择停止发送后续数据也可选择继续选择发送。而前者会造成数据的阻塞，后者会造成本应发往该Broker的数据的丢失。
- 如果Producer使用异步模式，则Producer会尝试重新发送`message.send.max.retries`（默认值为3）次后记录该异常并继续发送后续数据，这会造成数据丢失并且用户只能通过日志发现该问题。

　　由此可见，在没有Replication的情况下，一旦某机器宕机或者某个Broker停止工作则会造成整个系统的可用性降低。随着集群规模的增加，整个集群中出现该类异常的几率大大增加，因此对于生产系统而言Replication机制的引入非常重要。　　

#### 为何需要Leader Election

　　（本文所述Leader Election主要指Replica之间的Leader Election）
　　引入Replication之后，同一个Partition可能会有多个Replica，而这时需要在这些Replication之间选出一个Leader，Producer和Consumer只与这个Leader交互，其它Replica作为Follower从Leader中复制数据。
　　因为需要保证同一个Partition的多个Replica之间的数据一致性（其中一个宕机后其它Replica必须要能继续服务并且即不能造成数据重复也不能造成数据丢失）。如果没有一个Leader，所有Replica都可同时读/写数据，那就需要保证多个Replica之间互相（N×N条通路）同步数据，数据的一致性和有序性非常难保证，大大增加了Replication实现的复杂性，同时也增加了出现异常的几率。而引入Leader后，只有Leader负责数据读写，Follower只向Leader顺序Fetch数据（N条通路），系统更加简单且高效。　　　

#### 具体可参考：

http://www.sohu.com/a/190071430_411876

### Kafka的高性能体现

Kafka吞吐量是大家公认的高，那么这是为什么呢？**个人总结为以下三点：**

1，Broker NIO异步消息处理，实现了IO线程与业务线程分离。

2，磁盘顺序写。

3，零拷贝。

#### 总体架构

我们可以看到Kafka整个集群里面仅仅包含了**Broker**和**Zookeeper**两个组件。

Broker是整个Kafka集群的核心引擎，负责消息的存储转发，并对外提供服务。我们可以看到，Kafka集群可以非常简单的通过增删Broker，实现整个集群的扩缩容。Kafka对外提供服务的基本单位是Topic，那么实现Topic级别的平行扩展能力，也就实现了应用级的平行扩展能力。为了实现应用级的平行扩展能力，Kafka采用了对Topic进行分区的做法，通过对Topic进行分区让不同的分区落在不同的Broker上，从而利用到更多Broker的能力，最终实现了应用级的水平扩展。

Zookeeper则在整个集群中则主要负责存储一些配置信息、Broker信息、Topic信息等等元数据，并且承担了一部分协调选主的功能，可以将其理解为Kafka集群的配置管理中心。讲到这里，大家会觉得在Kafka集群中，可以简单的通过Broker动态的增删实现集群扩缩容，但在整个集群中却仅仅存在一个Zookeeper，那么Zookeeper会不会成为整个集群的瓶颈点，从而制约了整个集群的平行扩展能力？的确，在Kafka一些较老版本，Kafka的生产者及消费者都需要与Zookeeper进行通信交互，进行元数据拉取、消费分组协调、以及消费分组offset提交与保存等等。这样造成的一个问题，就是所有的客户端都要直接与ZooKeeper进行通讯交互，对其造成了非常大的压力，影响了系统的稳定性，最终影响整Kafka集群的平行扩展能力。但从0.9(含)版本之后，Kafka团队对整个Kafka进行了优化中，通过增加了一些协议，并且增加了协调模块。当前Kafka已经做到了客户端的生产及消费不需要与Zookeeper进行任何通讯交互，故Zookeeper当前仅仅充当了配置管理中心，压力非常的小，不会成为集群的瓶颈点进而制约集群的水平扩展能力。

大家也可以看到生产者及消费者是直接与Broker进行交互实现生产消费功能，Kafka在设计上并未采用传统系统中通过增加一层代理实现系统的平行扩展能力。Kafka在设计中通过内部路由协议，实现了生产者与消费者可以直接与Broker进行路由协商，从而实现了客户端直接与Broker进行生产消费，而不需要借助第三方代理。无代理的方式不仅会减少整个数据链路的长度，降低延迟，也可以提高整个系统的稳定性，而且也会节省大量的成本。

总结下Kafka的总体架构体现了如下几个主要的优势。其一，Kafka集群可以通过增删Broker实集群级的水平扩展。其二，通过对Topic进行分区，实现了应用级别的无限平行扩展能。其三，通过优良通讯协议，实现了生产系统直接与后端的Broker进行通讯，节省了代理，不仅减少了数据链路长度降低了延迟，而且极大的降低了成本。

至此，我想大家对Kafka已经有一个较为宏观了解。我们知道系统总体架构，决定了整个系统的能力上限。但系统中关键组件的性能，则是决定了相同能力下集群中服务器数量。服务器数量的增加，不仅仅会增加成本，而且会带来更多的运维压力及影响系统的稳定性。所以下面我将会介绍下Kafka核心引擎Broker的系统架构。

#### Broker架构

我们可以看Broker是一个典型的Reactor模型，其主要包含一个网络线程池，负责处理网络请求进行网络的收发以及打包解包，然后把请求通过请求队列推送给核心处理模块，由其负责真正的业务逻辑处理（Kafka会把所有消息落地存储，故主要是些文件I/0操作）。我们可以看到kafka采用多线程方式，可以充分利用现代系统的多核优势。其次，其采用队列方式实现了网络处理模块及核心处理模块的异步解耦，实现了网络处理和文件I/O并行处理，极大的提高了整个系统的效率。

讲到这里，大家对Kafka架构已经有一个宏观的理解。上面我们也提到，Kafka会把所有的消息都落地存储。那么为什么Kafka并未像传统的消息队列那样非常惧怕磁盘，劲量缓存而不触碰磁盘？Kafka为什么选择了将所有消息都落地存储？下面我将通过讲解其存储组织方式及存储格式，为大家进行一一揭秘。

#### 存储的组织方式

这个就是当前Kafka的存储组织方式，我们可以看到Topic，其实只是逻辑概念，并不对应任何物理实体，为了实现Topic的水平扩展，Kafka会对其进行分区。Partition则以目录形式进行展现，同时Partition中具体的数据还未进行分片存储。这样在生产时，就能快速的找到最新的分配直接追加到文件末尾，可以看到Kafka在生产中充分利用了磁盘的顺序写，极大的提高了生产的吞吐能力。同时进行分片存储，还有一个好处就是我们可以非常方便的通过删除老的分片实现过期消息的删除。同时为了方便消费，Kafka在分片的命名上也采用了一定的技巧，分片的命名采用了其包含的第一条消息的offset进行格式化这样，在消费数据时，可以非常方便的通过二分查找定位到消息所在的文件分片。同时为了实现分片内快速定位，Kafka也会对每个数据分片建立两个稀疏索引文件，通过索引文件，采用二分查找可以非常快速的定位了指定消息在数据分片中的位置，进而进行消费。通过讲解我们可以看到Kafka的整个生产消费其实都是顺序读写，充分利用了磁盘顺序读写能力。其次，Kafka的消费，采用了二级二分查找，其查找性能仅仅依赖一个分片的索引大小，不会受整个系统数据量的影响。

Kafka处理的最基本单位是消息，那么Kafka的消息具体是什么格式？落地存储的消息又具体是什么格式？不用说这些都将极大的影响系统的性能，所以下面我也将会详细的介绍下Kafka的消息格式。

#### 消息格式

为方便大家理解，这里我就以C代码的形式进行消息格式展示。Kafka消息其实是采用的简单的二进制编码，网络字节序存储，故可以非常高效的进行编解码操作。同时，我们可以看到整个Kafka消息头部非常的紧凑，仅仅只有30个字节左右，而且还包含了crc校验码用于对消息进行相关的校验。同时在Kafka中最为精妙的是，该消息的格式在生产系统端、网络传输中、Broker端，包括最终的文件存储中，都保持了消息格式的一致性。所以使得消息在整个系统的传输中，不需要有任何转码，效率奇高。当然为了提高整个系统的吞吐，Kafka这边实现了消息的批量生产消费。批量消息在Kafka中表现形式为，以二进制形式在内存中一个个排列起来。同时为了提高系统网络及磁盘的利用率，Kafka还是实现了消息压缩，右边这个流程图则详细的说明了消息压缩的流程。可以看到，首先我们把整个批量消息以一个整体进行压缩，生成一个新的二进制串。然后，把该值打包到一个新的消息的value字段中。可以看到Kafka非常巧妙的通过消息嵌套方式实现了消息的批量压缩，提高了整体压缩效率。同时该方式也保证了消息的格式的一致性。保持消息一致性就有以下好处:其一，我们在整个消息流转中仅仅需要生产者进行一次压缩后，将该压缩消息发送到Broker端后，Broker端仅需要一次解压操作，用以进行消息校验，并进行消息offset设置，后就能直接把消息直接存储在文件中，不需要有Broker进行一次非常消耗性能的压缩操作。所以说即使采用的消息压缩，对于Broker端的消耗也非常低。同时由于保持了压缩消息格式的一致性，当有消费请求时，Broker不用进行任何解压及压缩操作，可以直接将消息以压缩的方式发送给消费者，由消费者负责解压，这样的话Broker在整个消费中也就不需要任何的解压及压缩的操作，可以极大的提高Broker端的性能。可以看到Kafka通过这种方式，非常简单的实现了端到端的数据压缩，把计算资源消耗分摊到生产系统和消费系统中。

### kafka顺序磁盘读写

具体到Kafka而言，它使用了基于日志结构(log-structured)的数据格式，即每个分区日志只能在尾部追加写入(append)，而不允许随机“跳到”某个位置开始写入，故此实现了顺序写入。

可**参考：**：https://mp.weixin.qq.com/s/ssJV6wnH7fE2arZ07_Og4w

### kafka零拷贝技术

https://blog.csdn.net/lxlmycsdnfree/article/details/78973864

### **kafka**为什么要在topic里加入**分区**的概念？

topic是逻辑的概念，partition是物理的概念，对用户来说是透明的。producer只需要关心**消息**发往哪个topic，而consumer只关心自己订阅哪个topic，并不关心每条**消息**存于整个集群的哪个broker。

为了性能考虑，如果topic内的**消息**只存于一个broker，那这个broker会成为瓶颈，无法做到水平扩展。所以把topic内的数据分布到整个集群就是一个自然而然的设计方式。Partition的引入就是解决水平扩展问题的一个方案。

如同在[**Kafka**设计解析（一）**](https://link.zhihu.com/?target=http%3A//www.jasongj.com/2015/03/10/KafkaColumn1/)里所讲，每个partition可以被认为是一个无限长度的数组，新数据顺序追加进这个数组。物理上，每个partition对应于一个文件夹。一个broker上可以存放多个partition。这样，producer可以将数据发送给多个broker上的多个partition，consumer也可以并行从多个broker上的不同paritition上读数据，实现了水平扩展

### 如果没有**分区**,topic中的segment**消息**写满后,直接给订阅者不是也可以吗

“segment**消息**写满后”，consume消费数据并不需要等到segment写满，只要有一条数据被commit，就可以立马被消费

segment对应一个文件（实现上对应2个文件，一个数据文件，一个索引文件），一个partition对应一个文件夹，一个partition里理论上可以包含任意多个segment。所以partition可以认为是在segment上做了一层包装。

这个问题换个角度问可能更好，“为什么有了partition还需要segment”。
如果不引入segment，一个partition直接对应一个文件（应该说两个文件，一个数据文件，一个索引文件），那这个文件会一直增大。同时，在做data purge时，需要把文件的前面部分给删除，不符合**kafka**对文件的顺序写优化设计方案。引入segment后，每次做data purge，只需要把旧的segment整个文件删除即可，保证了每个segment的顺序写，

### **什么是消费者组**

https://www.cnblogs.com/rainwang/p/7496147.html

### **消息持久化**

 很多系统、组件为了提升效率一般恨不得把所有数据都扔到内存里，然后定期flush到磁盘上；可实际上，现代操作系统也是这样，所有的现代操作系统都乐于将空闲内存转作磁盘缓存（页面缓存），想不用都难；对于这样的系统，他的数据在内存中保存了一份，同时也在OS的页面缓存中保存了一份，这样不但多了一个步骤还让内存的使用率下降了一半；因此，Kafka决定直接使用页面缓存；但是随机写入的效率很慢，为了维护彼此的关系顺序还需要额外的操作和存储，而线性的写入可以避免这些，实际上，线性写入（linear write）的速度大约是300MB/秒，但随即写入却只有50k/秒，其中的差别接近10000倍。这样，Kafka以页面缓存为中间的设计在保证效率的同时还提供了消息的持久化，每个消费者自己维护当前读取数据的offser（也可委托给zookeeper），以此可同时支持在线和离线的消费。

### **消息投递可靠性**

一个消息如何算投递成功，Kafka提供了三种模式：

- 第一种是啥都不管，发送出去就当作成功，这种情况当然不能保证消息成功投递到broker；
- 第二种是Master-Slave模型，只有当Master和所有Slave都接收到消息时，才算投递成功，这种模型提供了最高的投递可靠性，但是损伤了性能；
- 第三种模型，即只要Master确认收到消息就算投递成功；实际使用时，根据应用特性选择，绝大多数情况下都会中和可靠性和性能选择第三种模型

   消息在broker上的可靠性，因为消息会持久化到磁盘上，所以如果正常stop一个broker，其上的数据不会丢失；但是如果不正常stop，可能会使存在页面缓存来不及写入磁盘的消息丢失，这可以通过配置flush页面缓存的周期、阈值缓解，但是同样会频繁的写磁盘会影响性能，又是一个选择题，根据实际情况配置。

   消息消费的可靠性，Kafka提供的是“At least once”模型，因为消息的读取进度由offset提供，offset可以由消费者自己维护也可以维护在zookeeper里，但是当消息消费后consumer挂掉，offset没有即时写回，就有可能发生重复读的情况，这种情况同样可以通过调整commit offset周期、阈值缓解，甚至消费者自己把消费和commit offset做成一个事务解决，但是如果你的应用不在乎重复消费，那就干脆不要解决，以换取最大的性能。

### kafka一致性保证

一致性定义：若某条消息对client可见，那么即使Leader挂了，在新Leader上数据依然可以被读到

1. HW-HighWaterMark: client可以从Leader读到的最大msg offset，即对外可见的最大offset， HW=max(replica.offset)
2. 对于Leader新收到的msg，client不能立刻消费，Leader会等待该消息被所有ISR中的replica同步后，更新HW，此时该消息才能被client消费，这样就保证了如果Leader fail，该消息仍然可以从新选举的Leader中获取。

对于来自内部Broker的读取请求，没有HW的限制。同时，Follower也会维护一份自己的HW，Folloer.HW = min(Leader.HW, Follower.offset)